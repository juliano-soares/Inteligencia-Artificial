{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rede-neural.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QcRBJsZCNW8P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision \n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() # definindo a conversão de imagem para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data', download=True, train=True, transform=transform) # Carrega a aparte de treino do dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data', download=True, train=False, transform=transform) # carrega a parte de validação\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) # Cria o buffer para pegar os dados por partes"
      ],
      "metadata": {
        "id": "7q5a0_pcPKMe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Zjpg4QLqQpe_",
        "outputId": "0c4a4228-22b5-4712-be11-b59ae675b42d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa8893c3a90>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) # para verificar as dimensões do tensor de cada imagem\n",
        "print(etiquetas[0].shape) # para verificar as dimensões do tensor de cada etiqueta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx-upT2kRPMF",
        "outputId": "977a0192-2c87-422c-982b-d8ded85140c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28, 128) # camada de entrada, 784 neurônios que se ligam a 128\n",
        "    self.linear2 = nn.Linear(128, 64) # camada interna 1, 128 neurônios que se ligam a 64\n",
        "    self.linear3 = nn.Linear(64, 10) # camda interna 2, 64 neurônios uqe se ligam a 10\n",
        "    # para a a camda de saida não é necessário definir nanda pois só precisamos pegar o output da camada interna 2\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = F.relu(self.linear1(X)) # função de ativação da camada de entrada para a a camada interna 1\n",
        "    X = F.relu(self.linear2(X)) # função de ativação da camda interna 1 para a camada interna 2\n",
        "    X = self.linear3(X) # função de ativação da camda interna 3 para a camada de saida, nesse caso f(x) = x \n",
        "    return F.log_softmax(X, dim=1) # dados utilizados para calcular a perda"
      ],
      "metadata": {
        "id": "lk032IfURrLV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treino(modelo, treinloader, device):\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) #define a politica de atualização dos pesos e da bias\n",
        "  inicio = time() # timer para sabermos quanto tempo levou o trino\n",
        "\n",
        "  criterio = nn.NLLLoss() # definindo o  criterio para calcular a perda\n",
        "  EPOCHS = 10 # numero de epochs que o algoritmo rodara\n",
        "  modelo.train() # ativado o modo de treinamento do modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 # inicialização da perda acumulada da epoch em questão\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "      imagens = imagens.view(imagens.shape[0], -1) # convertendo as imagens para \"vetores\" de 28*28 casas para ficarem compativeis com a \n",
        "      otimizador.zero_grad() # zerando os gradientes por conta do cilco anterior\n",
        "\n",
        "      output = modelo(imagens.to(device)) # colocando os dados no modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device)) # calculando a perda da epoch em questão\n",
        "\n",
        "      perda_instantanea.backward() # back propagation a partir da perda\n",
        "\n",
        "      otimizador.step() # atualizando os pesos e a bias\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item() # atualização da perda acumulada\n",
        "    \n",
        "    else:\n",
        "      print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "  print(\"\\nTempo de treino (em  minutos) =\",(time()-inicio)/60)"
      ],
      "metadata": {
        "id": "_byQbJ2ET7Ct"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0, 0\n",
        "  for imagens, etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[i].view(1, 784)\n",
        "      # desativar o autograd para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento\n",
        "      with torch.no_grad():\n",
        "        logps = modelo(img.to(device)) # output do modelo em escala logaritmica\n",
        "\n",
        "      ps = torch.exp(logps) # convertendo output para escala normal(lembrando que é um tensor)\n",
        "      probab = list(ps.cpu().numpy()[0])\n",
        "      etiqueta_pred = probab.index(max(probab)) # convertendo o tensor em um numero, no caso, o número que o modelo previu como correto\n",
        "      etiqueta_certa = etiquetas.numpy()[i]\n",
        "      if(etiqueta_certa == etiqueta_pred): # compara a previsão com o valor correto\n",
        "        conta_corretas += 1\n",
        "      conta_todas += 1\n",
        "\n",
        "  print(\"Total de imagens testadas =\", conta_todas)\n",
        "  print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))"
      ],
      "metadata": {
        "id": "DiFocI00WyMt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HO903PeYwUe",
        "outputId": "81ac6767-e002-43f2-9947-36950b80eda9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pCE5CiszZj8Q"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}